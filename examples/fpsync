#!/bin/sh

# Copyright (c) 2014 Ganael LAPLANCHE <ganael.laplanche@martymac.org>
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE AUTHORS AND CONTRIBUTORS ``AS IS'' AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHORS OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
# SUCH DAMAGE.

# This script is a simple wrapper showing how fpart can be used to migrate data.
# It uses fpart and rsync to spawn multiple rsync instances to migrate data
# from src_dir/ to dst_dir/. Rsync jobs can execute either locally or over SSH.
# This migration is incremental and will need a final rsync pass
# (rsync -av -delete src_dir/ dst_dir/) to remove extra files from dst_dir/.
# See the Configuration section below to set-up the script.

########## Helper functions

usage () {
    echo "fpsync - Start multiple rsync in parallel"
    echo "Usage: $0 [-n <x>] /src_dir/ /dst_dir/"
    echo "  -n <x>:    start <x> concurrent rsync workers"
    echo "  /src_dir/: source directory (absolute path)"
    echo "  /dst_dir/: destination directory (absolute path)"
    echo "Edit the script itself to fine-tune its configuration."
}

echo_log () {
    [ -n "$1" ] && [ -n "${VERBOSE}" ] && echo "$1"
    echo "$1" >> "${FPART_LOGFILE}"
}

end_die () {
    [ -n "$1" ] && echo -e "$1" 1>&2
    exit 1
}

is_abs_path() {
    echo "$1" | grep -qE "^/"
}

is_integer () {
    echo "$1" | grep -qE '^[0-9]+$'
}

# Handle ^C: stop queue processing by setting the "stop" flag
trap_queue () {
    job_queue_stop
    end_die "\nKilled."
}
trap 'trap_queue' 2

########## Arguments handling

# Handle -n option, if any
JOBS_MAX=4                              # How many jobs to run by default
#JOBS_MAX=$(($(sysctl -n hw.ncpu) - 1)) # The same, but autodetected (FreeBSD)
if [ "x$1" = "x-n" ]
then
    shift
    if is_integer "$1" && [ $1 -ge 1 ]
    then
        JOBS_MAX="$1"
    else
        end_die "Please supply a numeric value >= 1 for option -n"
    fi
    shift
fi

# Check for paths presence and validity
if is_abs_path "$1" && is_abs_path "$2"
then
    :
else
    usage
    end_die
fi
SRC_PATH="$1"
DST_PATH="$2"

# Read the job name
while read -p "Enter a job name: " FPART_JOBNAME && \
    echo "${FPART_JOBNAME}" | grep -vqE '^[a-zA-Z0-9]+$'
do
    :
done

########## Configuration

# Verbose mode - unset for quiet mode
VERBOSE="yes"

# Queue manager configuration. This queue remains local, even when using SSH.
JOBS_QUEUEDIR="/var/tmp/fpart/queue/${FPART_JOBNAME}"   # Queue dir.
JOBS_WORKDIR="/var/tmp/fpart/work/${FPART_JOBNAME}"     # Current jobs' dir.

# Paths to executables that must exist locally
FPART_BIN="/usr/local/bin/fpart"
SSH_BIN="/usr/bin/ssh"
MAIL_BIN="/usr/bin/mail"
# This script also uses cut, sed and awk, which should come with your system

# Paths to executables that must exist either locally or remotely (depending
# on if you use SSH or not). When using SSH, those binaries must be present at
# those paths on each worker.
RSYNC_BIN="/usr/local/bin/rsync"

# Fpart paths. Those ones must be shared amongst all nodes when using SSH
# (e.g. through a NFS share mounted on *every* single node, including the master
# 'job submitter').
FPART_LOGDIR="/mnt/nfs/fpart/log/${FPART_JOBNAME}"
FPART_LOGFILE="${FPART_LOGDIR}/fpart.log"
FPART_OUTPARTDIR="/mnt/nfs/fpart/partitions/${FPART_JOBNAME}"

# Fpart options / splitting thresholds
FPART_MAXPARTFILES="2000"
FPART_MAXPARTSIZE="$((4 * 1024 * 1024 * 1024))" # 4 GB
FPART_OPTIONS="-x '.zfs' -x '.snapshot*'"

# Fpart hooks
FPART_COMMAND="/bin/sh -c '${RSYNC_BIN} -av --numeric-ids \
        --files-from=\\\"\${FPART_PARTFILENAME}\\\" \
        \\\"${SRC_PATH}/\\\" \
        \\\"${DST_PATH}/\\\"' \
        1>\"${FPART_LOGDIR}/$$-\${FPART_PARTNUMBER}.stdout\" \
        2>\"${FPART_LOGDIR}/$$-\${FPART_PARTNUMBER}.stderr\""
FPART_POSTHOOK="echo \"${FPART_COMMAND}\" > \
        \"${JOBS_QUEUEDIR}/\${FPART_PARTNUMBER}\" && \
        [ -n \"${VERBOSE}\" ] && \
        echo \"=> [FPART] Partition \${FPART_PARTNUMBER} written\"" # [1]

# [1] Be careful to host the job queue on a filesystem that can handle
# fine-grained mtime timestamps (i.e. with a sub-second precision) if you want
# the queue to be processed in order when fpart generates several job files per
# second.
# On FreeBSD, vfs timestamps' precision can be tuned using the
# vfs.timestamp_precision sysctl. See vfs_timestamp(9).

# Mail - Uncomment to receive a mail when the whole rsync job has finished.
# The master machine (the one running this script) must be able to send mail
# using the 'mail' command.
#MAIL_ADDR="address@mydomain.tld"

# List of SSH hosts (space-separated) to use as rsync workers (nodes).
# If unset, jobs will execute locally, else, jobs will be sent through SSH
# and executed remotely. In that case, be sure to set up passwordless access for
# those accounts so that rsync jobs can execute seemlessly.
# See ssh(1) for more details.
#SSH_HOSTS="login@host1 login@host2 login@host3"

########## Work-related functions (in-memory, running-jobs handling)

# Initialize WORK_FREEWORKERS by expanding SSH_HOSTS up to JOBS_MAX elements,
# assigning a fixed number of slots to each worker.
# Sanitize SSH_HOSTS if necessary.
work_list_free_workers_init () {
    local _SSH_HOSTS_NUM=$(echo ${SSH_HOSTS} | awk '{print NF}')
    if [ ${_SSH_HOSTS_NUM} -gt 0 ]
    then
        local _i=0
        while [ ${_i} -lt ${JOBS_MAX} ]
        do
            local _SSH_HOSTS_IDX="$((${_i} % ${_SSH_HOSTS_NUM} + 1))"
            WORK_FREEWORKERS="${WORK_FREEWORKERS} $(echo ${SSH_HOSTS} | awk '{print $'${_SSH_HOSTS_IDX}'}')"
            _i=$((${_i} + 1))
        done
    else
        SSH_HOSTS=""
        WORK_FREEWORKERS="local"
    fi
}

# Pick-up next worker
work_list_pick_next_free_worker () {
    echo "${WORK_FREEWORKERS}" | awk '{print $1}'
}

# Remove next worker from list
work_list_trunc_next_free_worker () {
    WORK_FREEWORKERS="$( echo ${WORK_FREEWORKERS} | sed -E 's/^[[:space:]]*[^[:space:]]+[[:space:]]*//')"
}

# Push a work to the list of currently-running ones
work_list_push () {
    if [ -n "$1" ]
    then
        WORK_LIST="${WORK_LIST} $1"
        WORK_NUM="$((${WORK_NUM} + 1))"
    fi
}

# Rebuild the currently-running jobs' list by examining each process' state
work_list_refresh () {
    local _WORK_LIST=""
    local _WORK_NUM=0
    for _JOB in ${WORK_LIST}
    do
        # If the process is still alive, keep it
        if ps "$(echo ${_JOB} | cut -d ':' -f 1)" 1>/dev/null 2>&1
        then
            _WORK_LIST="${_WORK_LIST} ${_JOB}"
            _WORK_NUM="$((${_WORK_NUM} + 1))"
        # If not, put its worker to the free list
        else
            echo_log "<= [QMGR] Job ${_JOB} finished"
            if [ -n "${SSH_HOSTS}" ]
            then
                WORK_FREEWORKERS="${WORK_FREEWORKERS} $(echo ${_JOB} | cut -d ':' -f 2)"
            fi
        fi
    done
    WORK_LIST=${_WORK_LIST}
    WORK_NUM=${_WORK_NUM}
}

########## Jobs-related functions (on-disk, jobs' queue handling)

# Initialize job queue and work directories
job_queue_init () {
    mkdir -p "${JOBS_QUEUEDIR}" 2>/dev/null || \
        end_die "Cannot create job queue directory ${JOBS_QUEUEDIR}"
    rm -f "${JOBS_QUEUEDIR}"/*
    mkdir -p "${JOBS_WORKDIR}" 2>/dev/null || \
        end_die "Cannot create job work directory ${JOBS_WORKDIR}"
    rm -f "${JOBS_WORKDIR}"/*
}

# Set the "done" flag within job queue
job_queue_done () {
    sleep 1 # Ensure this very last file gets created within the next second of
            # last job file's mtime. Necessary for filesystems that don't get
            # below the second for mtime precision (msdosfs).
    touch "${JOBS_QUEUEDIR}/done"
}

# Set the "stop" flag within job queue
job_queue_stop () {
    touch "${JOBS_QUEUEDIR}/stop"
}

# Get next job name relative to ${JOBS_WORKDIR}/
# Returns empty string if no job is available
job_queue_next () {
    local _NEXT=""
    if [ -f "${JOBS_QUEUEDIR}/stop" ]
    then
        echo "stop"
    else
        _NEXT=$(cd "${JOBS_QUEUEDIR}" && ls -rt1 | head -n 1)
        if [ -n "${_NEXT}" ]
        then
            mv "${JOBS_QUEUEDIR}/${_NEXT}" "${JOBS_WORKDIR}" || \
                end_die "Cannot dequeue next job"
            echo "${_NEXT}"
        fi
    fi
}

# Main jobs' loop: pick up jobs within the queue directory and start them
job_queue_loop () {
    local _NEXT=""
    while [ "${_NEXT}" != "done" ] && [ "${_NEXT}" != "stop" ]
    do
        local _PID=""
        if [ ${WORK_NUM} -lt ${JOBS_MAX} ]
        then
            _NEXT="$(job_queue_next)"
            if [ -n "${_NEXT}" ] && \
                [ "${_NEXT}" != "done" ] && \
                [ "${_NEXT}" != "stop" ]
            then
                if [ -z "${SSH_HOSTS}" ]
                then
                    echo_log "=> [QMGR] Starting job ${JOBS_WORKDIR}/${_NEXT} (local)"
                    /bin/sh "${JOBS_WORKDIR}/${_NEXT}" &
                    work_list_push "$!:local"
                else
                    local _NEXT_HOST="$(work_list_pick_next_free_worker)"
                    work_list_trunc_next_free_worker
                    echo_log "=> [QMGR] Starting job ${JOBS_WORKDIR}/${_NEXT} -> ${_NEXT_HOST}"
                    "${SSH_BIN}" "${_NEXT_HOST}" '/bin/sh -s' \
                        < "${JOBS_WORKDIR}/${_NEXT}" &
                    work_list_push "$!:${_NEXT_HOST}"
                fi
            fi
        fi
        work_list_refresh
        sleep 0.2
    done

    if [ "${_NEXT}" == "done" ]
    then
        echo_log "=> [QMGR] Done submitting jobs. Waiting for them to finish."
    else
        echo_log "=> [QMGR] Stopped. Waiting for jobs to finish."
    fi
    wait
    echo_log "=> [QMGR] No more job running."
}

########## Program start

FPART_OUTPARTTEMPL="${FPART_OUTPARTDIR}/part-$$"

WORK_NUM=0          # Current number of running processes
WORK_LIST=""        # Work PID[:WORKER] list
WORK_FREEWORKERS="" # Free workers' list

# Check for essential binaries
if [ ! -x "${FPART_BIN}" ] || [ ! -x "${SSH_BIN}" ] || [ ! -x "${MAIL_BIN}" ]
then
    end_die "External tools are missing, check your configuration"
fi

# Create fpart directories and log file
mkdir -p "${FPART_OUTPARTDIR}" 2>/dev/null || \
    end_die "Cannot create partitions' output directory: ${FPART_OUTPARTDIR}"
mkdir -p "${FPART_LOGDIR}" 2>/dev/null || \
    end_die "Cannot create log directory: ${FPART_LOGDIR}"
touch "${FPART_LOGFILE}" 2>/dev/null || \
    end_die "Cannot create log file: ${FPART_LOGFILE}"

# Validate src_dir/ locally (needed for fpart)
[ ! -d "${SRC_PATH}" ] && \
    end_die "Directory ${SRC_PATH} does not exist (or is not a directory)"

# When using SSH, validate src_dir/ and dst_dir/ remotely and check for rsync
# presence (this also allows checking SSH connectivity to each declared host)
if [ -n "${SSH_HOSTS}" ]
then
    echo_log "======> Validating requirements on SSH nodes..."
    for _host in ${SSH_HOSTS}
    do
        "${SSH_BIN}" "${_host}" "/bin/sh -c '[ -d \"${SRC_PATH}\" ]'" || \
            end_die "Directory ${SRC_PATH} does not exist on target ${_host} (or is not a directory)"
        "${SSH_BIN}" "${_host}" "/bin/sh -c '[ -d \"${DST_PATH}\" ]'" || \
            end_die "Directory ${DST_PATH} does not exist on target ${_host} (or is not a directory)"
        "${SSH_BIN}" "${_host}" "/bin/sh -c '[ -x \"${RSYNC_BIN}\" ]'" || \
            end_die "Rsync is missing on target ${_host}, check your configuration"
        echo_log "===> ${_host}: OK"
    done
else
    # Local usage - check for dst_dir/ and rsync presence
    [ ! -d "${DST_PATH}" ] && \
        end_die "Directory ${DST_PATH} does not exist (or is not a directory)"
    [ ! -x "${RSYNC_BIN}" ] && \
        end_die "Rsync is missing locally, check your configuration"
fi

# Dispatch SSH_HOSTS into WORK_FREEWORKERS
work_list_free_workers_init

# Initialize jobs queue and start job_queue_loop
job_queue_init
job_queue_loop&

# Let's rock !
echo_log "======> [$$] Syncing ${SRC_PATH} => ${DST_PATH}"
echo_log "===> Start time: $(date)"
echo_log "===> Starting fpart..."
echo_log "===> (parts dir: ${FPART_OUTPARTDIR})"
echo_log "===> (log dir: ${FPART_LOGDIR})"

# Start fpart from src_dir/ directory and produce jobs within ${JOBS_QUEUEDIR}/
cd "${SRC_PATH}" && \
    "${FPART_BIN}" -f "${FPART_MAXPARTFILES}" -s "${FPART_MAXPARTSIZE}" \
        -o "${FPART_OUTPARTTEMPL}" ${FPART_OPTIONS} -Z -L \
        -W "${FPART_POSTHOOK}" . 2>&1 | tee -a "${FPART_LOGFILE}"

# Tell job_queue_loop that crawling has finished
job_queue_done

# Wait for job_queue_loop to terminate
echo_log "===> Jobs submitted, waiting..."
wait

# Examine results and send an e-mail if requested
RET=$(find "${FPART_LOGDIR}/" -name "$$-*.stderr" ! -size 0)
MSG=$(echo -e "Return code: $( ([ -z "${RET}" ] && echo '0') || echo '1')" ; \
    [ -n "${RET}" ] && echo -e "Rsync errors detected, see logs:\n${RET}")
if [ -n "${MAIL_ADDR}" ]
then
    echo "${MSG}" | ${MAIL_BIN} -s "Fpart job ${FPART_JOBNAME}" "${MAIL_ADDR}"
fi
echo_log "===> Jobs terminated."
echo "${MSG}" | tee -a "${FPART_LOGFILE}"
echo_log "<=== End time: $(date)"

[ -n "${RET}" ] && exit 1
exit 0
